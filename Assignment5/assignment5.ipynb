{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "import csv\n",
    "from pyspark.sql.functions import desc, avg, split, countDistinct, explode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import split, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 117 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 44.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764020 sha256=dd12afc3f0100806a6ad8e6c48a735050b0cd279f6c9a5eac03de83c9d193879\n",
      "  Stored in directory: /homes/mjalali/.cache/pip/wheels/05/75/73/81f84d174299abca38dd6a06a5b98b08ae25fce50ab8986fa1\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/26 22:51:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/26 22:51:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=66>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/pyspark/context.py\", line 362, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/pyspark/context.py\", line 1447, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/homes/mjalali/.local/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o31.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000002vscode-remote?line=0'>1</a>\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/data/dataprocessing/interproscan/all_bacilli.tsv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000002vscode-remote?line=1'>2</a>\u001b[0m sc\u001b[39m=\u001b[39mSparkContext(\u001b[39m'\u001b[39m\u001b[39mlocal[16]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000002vscode-remote?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m SQLContext(sc)\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(file, sep\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, inferSchema\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000002vscode-remote?line=3'>4</a>\u001b[0m df\u001b[39m.\u001b[39mshow(\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=532'>533</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=533'>534</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=534'>535</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=535'>536</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=537'>538</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/utils.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/utils.py?line=188'>189</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/utils.py?line=189'>190</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/utils.py?line=190'>191</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/utils.py?line=191'>192</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=332'>333</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=333'>334</a>\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=334'>335</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=335'>336</a>\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=336'>337</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/py4j/protocol.py?line=337'>338</a>\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o31.csv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file = '/data/dataprocessing/interproscan/all_bacilli.tsv'\n",
    "sc=SparkContext('local[16]')\n",
    "df = SQLContext(sc).read.csv(file, sep=r'\\t', header=False, inferSchema= True)\n",
    "df.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/27 14:42:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+-------+---------+--------------------+---+---+-------+---+----------+---------+--------------------+----+----+\n",
      "|                 _c0|                 _c1|_c2|    _c3|      _c4|                 _c5|_c6|_c7|    _c8|_c9|      _c10|     _c11|                _c12|_c13|_c14|\n",
      "+--------------------+--------------------+---+-------+---------+--------------------+---+---+-------+---+----------+---------+--------------------+----+----+\n",
      "|gi|29898682|gb|AA...|92d1264e347e14924...|547|TIGRFAM|TIGR03882|cyclo_dehyd_2: ba...|  2|131|1.6E-21|  T|25-04-2022|IPR022291|Bacteriocin biosy...|   -|   -|\n",
      "|gi|29898682|gb|AA...|92d1264e347e14924...|547|TIGRFAM|TIGR03604|TOMM_cyclo_SagD: ...|161|547|    0.0|  T|25-04-2022|IPR027624|Thiazole/oxazole-...|   -|   -|\n",
      "+--------------------+--------------------+---+-------+---------+--------------------+---+---+-------+---+----------+---------+--------------------+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[16]\").getOrCreate()\n",
    "file = '/data/dataprocessing/interproscan/all_bacilli.tsv'\n",
    "df = spark.read.csv(file, sep=r'\\t', header=False, inferSchema= True)\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:====================================================>  (80 + 4) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT _c11)|\n",
      "+--------------------+\n",
      "|                9704|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# distinct protein annotations\n",
    "dpa = df.select(countDistinct(\"_c11\"))\n",
    "dpa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT _c4)|\n",
      "+-------------------+\n",
      "|              24488|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4200591"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct protein annotations\n",
    "dpa = df.select(countDistinct(\"_c4\"))\n",
    "dpa.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:======================================================> (81 + 3) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.490524386574391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = df.groupBy(\"_c0\").count().agg({'count':'mean'})\n",
    "result = df2.take(1)[0]['avg(count)']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:======================================================> (81 + 3) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO:0005524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = df.filter(\"_c13 != '-'\")\n",
    "df2 = df2.select(split(col(\"_c13\"),\"\\|\").alias('GO'))\n",
    "df2 = df2.select(explode(df2.GO))\n",
    "df2 = df2.groupBy(\"col\").count().sort(col(\"count\"), ascending=False)\n",
    "result = df2.take(1)[0]['col']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:======================================================> (81 + 3) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.73178916966685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = df.select((col(\"_c7\") - col(\"_c6\")).alias('Diff'))\n",
    "df2 = df2.agg({'Diff' : 'mean'})\n",
    "result = df2.take(1)[0]['avg(Diff)']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:=====================================================> (82 + 2) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IPR027417', 'IPR002347', 'IPR003439', 'IPR036388', 'IPR036259', 'IPR003593', 'IPR036390', 'IPR036291', 'IPR000515', 'IPR001789']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = df.filter(\"_c11 != '-'\")\n",
    "df2 = df2.groupBy(\"_c11\").count().sort(col(\"count\"), ascending=False)\n",
    "result = []\n",
    "for res in df2.take(10):\n",
    "    result.append(res[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:===============================================>      (74 + 10) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IPR006308', 'IPR006308', 'IPR006308', 'IPR006308', 'IPR006308', 'IPR012103', 'IPR006308', 'IPR006308', 'IPR006308', 'IPR006308']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#6\n",
    "df2 = df.filter(\"_c11 != '-'\")\n",
    "df2 = df2.select(\"_c11\",((col(\"_c7\")-col(\"_c6\"))/col(\"_c2\")).alias(\"Pro\"))\n",
    "df2 = df2.filter(\"Pro >= 0.9\").sort(\"Pro\", ascending=False)\n",
    "result = []\n",
    "for res in df2.take(10):\n",
    "    result.append(res[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:=====================================================> (81 + 3) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['domain', 'like', 'superfamily', 'protein', 'binding', 'terminal', 'type', 'C', 'DNA', 'hydrolase']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#7\n",
    "df2 = df.filter(\"_c12 != '-'\")\n",
    "df2 = df2.select(split(col(\"_c12\"),\"\\/| |-|,\").alias('W'))\n",
    "df2 = df2.select(explode(df2.W))\n",
    "df2 = df2.filter(\"col != ''\")\n",
    "df2 = df2.groupBy(\"col\").count().sort(col(\"count\"), ascending=False)\n",
    "result = []\n",
    "for res in df2.take(10):\n",
    "    result.append(res[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:========================================================>(83 + 1) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 col|count|\n",
      "+--------------------+-----+\n",
      "|        CHLOROPLASTS|    1|\n",
      "|Phycocyanobilin:f...|    1|\n",
      "|             DUF2555|    1|\n",
      "|             DUF4917|    1|\n",
      "|               Ycf34|    1|\n",
      "|                CsgG|    1|\n",
      "|             sll0784|    1|\n",
      "|               Curli|    1|\n",
      "|           UCP020893|    1|\n",
      "|               PICOT|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#8\n",
    "df2 = df.filter(\"_c12 != '-'\")\n",
    "df2 = df2.select(split(col(\"_c12\"),\"\\/| |-|,\").alias('W'))\n",
    "df2 = df2.select(explode(df2.W))\n",
    "df2 = df2.filter(\"col != ''\")\n",
    "df2 = df2.groupBy(\"col\").count().sort(col(\"count\"), ascending=True)\n",
    "result = []\n",
    "for res in df2.take(10):\n",
    "    result.append(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=======================================>             (120 + 40) / 160]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       col|count|\n",
      "+----------+-----+\n",
      "|       III|    9|\n",
      "|polymerase|    9|\n",
      "|      Gram|    9|\n",
      "|  positive|    9|\n",
      "|      type|    9|\n",
      "|     alpha|    9|\n",
      "|   subunit|    9|\n",
      "|       DNA|    9|\n",
      "| Peptidase|    1|\n",
      "|       S8A|    1|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#9\n",
    "df2 = df.filter(\"_c11 != '-'\")\n",
    "df2 = df2.select(\"_c11\", \"_c12\",((col(\"_c7\")-col(\"_c6\"))/col(\"_c2\")).alias(\"Pro\"))\n",
    "df2 = df2.filter(\"Pro >= 0.9\").sort(\"Pro\", ascending=False)\n",
    "df2 = df2.limit(10)\n",
    "df2 = df2.filter(\"_c12 != '-'\")\n",
    "df2 = df2.select(split(col(\"_c12\"),\"\\/| |-|,\").alias('W'))\n",
    "df2 = df2.select(explode(df2.W))\n",
    "df2 = df2.filter(\"col != ''\")\n",
    "df2 = df2.groupBy(\"col\").count().sort(col(\"count\"), ascending=False)\n",
    "result = []\n",
    "for res in df2.take(10):\n",
    "    result.append(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#10\n",
    "df2 = df.filter(\"_c11 != '-'\")\n",
    "df2 = df2.select(\"_c0\", \"_c11\", \"_c2\")\n",
    "df2 = df2.groupBy(\"_c0\", \"_c2\").agg({\"_c11\":\"count\"})\n",
    "\n",
    "R2 = df2.stat.corr(\"_c2\", \"count(_c11)\") ** 2\n",
    "output = []\n",
    "result = R2\n",
    "output.append([10, result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(file)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m writer\u001b[39m.\u001b[39mwriterow([\u001b[39m'\u001b[39m\u001b[39mQuestion Number\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mResult\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000013vscode-remote?line=4'>5</a>\u001b[0m     writer\u001b[39m.\u001b[39mwriterow(res)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "with open(\"output.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Question Number', 'Result'])\n",
    "    for res in output:\n",
    "        writer.writerow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show, collect, avg, distictcount, agg => aggregate, select, sort, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'InterPro_accession'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m A1 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mfilter(df\u001b[39m.\u001b[39;49mInterPro_accession\u001b[39m!=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mselect(countDistinct(\u001b[39m\"\u001b[39m\u001b[39mInterPro_accession\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix/homes/mjalali/Documents/programming3/Assignment5/assignment5.ipynb#ch0000006vscode-remote?line=1'>2</a>\u001b[0m A1\u001b[39m.\u001b[39mshow(\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1977'>1978</a>\u001b[0m \u001b[39m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1978'>1979</a>\u001b[0m \n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1979'>1980</a>\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1984'>1985</a>\u001b[0m \u001b[39m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1985'>1986</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1986'>1987</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m-> <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1987'>1988</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1988'>1989</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name)\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1989'>1990</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1990'>1991</a>\u001b[0m jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mapply(name)\n\u001b[1;32m   <a href='file:///homes/mjalali/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py?line=1991'>1992</a>\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'InterPro_accession'"
     ]
    }
   ],
   "source": [
    "A1 = df.filter(df.InterPro_accession!=\"-\").select(countDistinct(\"InterPro_accession\"))\n",
    "A1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_1 = A1.collect()[0][0]\n",
    "A1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_2 = A1._sc._jvm.PythonSQLUtils.explainString(A1._jdf.queryExecution(), 'simple')\n",
    "A1_2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
